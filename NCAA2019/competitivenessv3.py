# -*- coding: utf-8 -*-
"""competitivenessV3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_eD-J9jFGOV38i_AU_BWyZoEobKXiJ9G

##Using the following formula for competitiveness:


**V.2.0**

https://www.overleaf.com/read/wpftpkhqbjgw

Collaboration between Luis Ulloa, Jose Eduardo, Jenny Xue, Jack Van Boening, and Quang Nguyen

**V3.0**

https://www.overleaf.com/read/wpftpkhqbjgw

Changed definition of 'competitive bucket' to better reflect meaning of stat. In previous version, term of 'competitive bucket' was too lose of a definition (i.e. worst team was still competitive 60% of time). 

Added schedule adjustment ratings using Alok Pattani's colab:
https://colab.research.google.com/drive/13L4b36cTrnC55ahD6dVf4-r9pzkYV-j5#scrollTo=rg4UNqQSebYz
"""

#@title Import Python Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import statistics as st
import scipy.stats as stats
from sklearn.metrics import r2_score
import matplotlib.mlab as mlab
from tqdm import tqdm
import time
import operator
# Import linear model and pre-processing modules from scikit-learn
from sklearn import linear_model, preprocessing
warnings.filterwarnings("ignore")

team_seasons = pd.DataFrame(columns=['season', 'tm_code', 'opp_code', 'tm_hca', 'competitiveness'])

#@title Provide Google Credentials
from google.colab import auth
auth.authenticate_user()
print("Authenticated")

#@title Enter BigQuery Project
project_id = "stardust-analysis" #@param{type: "string"}

#Initialize BigQuery Client
from google.cloud import bigquery
client = bigquery.Client(project_id)

#@title Dict for Team Code to School Name
all_teams = client.query(
    """
      SELECT
        DISTINCT(team_code),
        school
      FROM
        `stardust-analysis.ncaa_mbb.team_info`
      WHERE
        division=1
      ORDER BY
        team_code ASC
    """).to_dataframe()

team_code_to_school = {}
for row, data in all_teams.iterrows():
  team_code_to_school[data["team_code"]] = data["school"]

"""## Raw Metric"""

## Avg Scoring Margin for all Buckets per Season
    total_avg = []
    total_per_bucket = 0
    for bucket in range(10):
      bottom_sec = bucket * 4 * 60
      top_sec = (bucket + 1) * 4 * 60
      total_per_bucket = 0
      score_margin = client.query(
            """
          SELECT
            pbp.game_id,
            elapsed_time_sec,
            home_pts,
            away_pts,
            (home_pts - away_pts) as score_margin
          FROM
            `stardust-analysis.cloude.pbp` AS pbp
          JOIN 
          `stardust-analysis.ncaa_mbb.team_box` AS team_box
          ON 
           team_box.game_id = pbp.game_id
          JOIN
        `stardust-analysis.ncaa_mbb.team_info` AS team
          ON
          team.team_code = team_box.team_code
          JOIN
         `stardust-analysis.ncaa_mbb.team_info` AS opp
        ON
         team_box.opp_code = opp.team_code
          WHERE
            team_box.season = 2018 AND
            team.division = 1 AND
            opp.division = 1 AND
            elapsed_time_sec > %d AND
            elapsed_time_sec <= %d AND
            event_type = "GOOD"
        GROUP BY
            game_id, elapsed_time_sec, home_pts, away_pts, score_margin
         ORDER BY
            game_id DESC,
            elapsed_time_sec DESC
            """ % (bottom_sec, top_sec)
        ).to_dataframe()

      tm_dict = dict()
      game_ids = []
      for i in score_margin.index:
        game_id  = score_margin.loc[i, 'game_id']
        if game_id in game_ids:
          continue
        else:
          game_ids.append(game_id)

        # Get first and last scores of bucket
        j = i + 1
        while (game_id == score_margin.loc[j, 'game_id']):
          if (j + 1 >= len(score_margin)):
            break
          j += 1
        j -= 1

        # Add Margins
        home_tm = game_id.split('-')[0]
        away_tm = game_id.split('-')[1]

        if bucket != 0:
          margin = score_margin.loc[i, 'score_margin'] - score_margin.loc[j, 'score_margin']
        else:
          margin = score_margin.loc[i, 'score_margin']

        total_per_bucket += abs(margin)

      total_avg.append(total_per_bucket / len(game_ids))

    averageScoreDiffForAllTeams = st.mean(total_avg)

def get_data(client, team_code):
  query = """
      SELECT
        game_id,
        is_home,
        elapsed_time_sec,
        home_pts,
        away_pts,
        points_scored,
        team_code
      FROM
        `stardust-analysis.cloude.pbp`
      WHERE
        points_scored != 0
        AND game_id IN (
        SELECT
          DISTINCT game_id
        FROM
          `stardust-analysis.ncaa_mbb.games` AS games
        WHERE
          games.season = 2018 #@param 
          AND (games.home_code = %s OR
          games.away_code = %s) AND
          games.home_division = 1 AND 
          games.away_division = 1
        )
      ORDER BY
        game_id ASC,
        elapsed_time_sec ASC
    """ % (team_code, team_code)
  games_df = client.query(query).to_dataframe()
  return games_df

def get_metric_values(client, games_df):
  """
  Returns count of winning segments and count of total segments
  for the team specified in games_df.
  
  """
  specifiedCount = 0
  totalCount = 0
  game_ids = set(games_df["game_id"])
  for game_id in game_ids:
    game_segments_score_diff = [0]*10 # 10 buckets, each 240
    game_total_score_diff = [0]*10 # 10 buckets, each 240

    is_home = False
    for row, data in games_df[games_df["game_id"] == game_id].iterrows():
      time_segments = ((data["elapsed_time_sec"]) - 1) // 240 # -1 handles 2400//240=10 indexing error
      if time_segments >= 10: # currently ignoring overtime periods, #TODO:don't
        continue
      
      points_scored = data["points_scored"]
      home_pts = data["home_pts"]
      away_pts = data["away_pts"]
      
      game_total_score_diff[time_segment] = home_pts - away_pts
      
      if points_scored != 0:
        if data["team_code"] == team_code:
          game_segments_score_diff[time_segments] += points_scored
          
        else:
          game_segments_score_diff[time_segments] -= points_scored

      # technically should only have to be done once
      if data["team_code"] == team_code and data["is_home"]:
        is_home = True
        
        
    if not is_home:
      for score in game_total_score_diff: 
        score *= -1
        
    # competitiveSegment = score diff > AND game_score_diff > -20
    # Loop through each segment and determine if it is a 'comepteitiveBucket'
    gm_count = 0
    for i in range(10):
      # Check segment score
      if (game_segments_score_diff[i] > 0):
        # Check game score
        if game_total_score_diff[i] > -20:
          specifiedCount += 1
      totalCount += 1
    
  return specifiedCount, totalCount

# see latex in first text block of this collab
def calculate_metric(team_data_frame):
    num_winning_buckets, num_total_buckets = get_metric_values(client, team_data_frame)
    
    if num_total_buckets == 0:
      return 0
    
    return (100 * (num_winning_buckets / num_total_buckets))

NUM_TEAMS = 353

raw_rankings = {}
for team_code in tqdm(team_code_to_school.keys()):
  team_data_frame = get_data(client, team_code)
  raw_rankings[team_code] = calculate_metric(team_data_frame)

SPACE_SIZE = 50
print("School", " "*(SPACE_SIZE - len("School")), "Competitive score")
rank = 1
for team_code, score in sorted(raw_rankings.items(), key=operator.itemgetter(1), reverse=True):  
  team_name = team_code_to_school[team_code]
  spacing = SPACE_SIZE - len(team_name) - len(str(rank))
  print(rank, team_name, " "*spacing , score)
  rank += 1

scores = list(raw_rankings.values())

plt.title("Frequency of each competitiveness score")
plt.xlabel("Competitiveness score")
plt.ylabel("Frequency")
plt.grid(True)
plt.hist(scores)

print("     mean: %0.4f   var: %0.4f" % (np.mean(scores), np.var(scores, ddof=1)))
print("     min: %0.4f    max: %0.4f" % (np.min(scores), np.max(scores)))

"""##Schedule Adjusted Metric"""

#@title Get game data for competitiveness score
#Initialize BigQuery Client
from google.cloud import bigquery
client = bigquery.Client(project_id)
query = """
    SELECT
      pbp.game_id,
      pbp.is_home,
      pbp.is_neutral,
      elapsed_time_sec,
      home_pts,
      away_pts,
      points_scored,
      team_code
    FROM
      `stardust-analysis.ncaa_mbb.pbp` pbp
    JOIN
      `stardust-analysis.ncaa_mbb.games` AS games
    ON
      pbp.game_id = games.game_id
      AND games.home_division = 1
      AND games.away_division = 1
      AND games.season = 2018
    WHERE
      points_scored != 0
    ORDER BY
      pbp.game_id ASC,
  elapsed_time_sec ASC
    """
games_df = client.query(query).to_dataframe()

#@title Calculate Competitivness Score for each Game
team_games = pd.DataFrame(columns=['season', 'tm_code', 'opp_code', 'tm_hca', 'competitiveness'])
index = 0

game_ids = set(games_df["game_id"])
for game_id in tqdm(game_ids):
  is_neutral = 0
  # Based off home team
  # i.e margin is pos if home team winning
  game_bucket_score_diff = [0]*10 # 10 buckets, each 240
  game_total_score_diff = [0]*10 # 10 buckets, each 240
  score_max_in_each_bucket = [{"home":0, "away":0}]*10

  for row, data in games_df[games_df["game_id"] == game_id].iterrows():
    time_bucket = ((data["elapsed_time_sec"]) - 1) // 240 # -1 handles 2400//240=10 indexing error
    if time_bucket >= 10: # currently ignoring overtime periods, #TODO:don't
      continue

    points_scored = data["points_scored"]
    home_pts = data["home_pts"]
    away_pts = data["away_pts"]

    game_total_score_diff[time_bucket] = home_pts - away_pts

    if points_scored != 0:
      if data["is_home"] == 1:
        game_bucket_score_diff[time_bucket] += points_scored

      else:
        game_bucket_score_diff[time_bucket] -= points_scored

    if (data['is_neutral'] == 'Y'):
      is_neutral = 1
  # at the end of the game, only increment respective season avg buckets
  # if it adheres to our restrictions      
  # competitiveBucket = score diff > 0 OR abs(score diff) < (avg for that bucket + stdev) AND game_score_diff > -20
  # Loop through each bucket and determine if it is a 'comepteitiveBucket'
  
  # Home team
  hm_count = 0
  for i in range(10):
    # Check bucket
    if (game_bucket_score_diff[i] > 0):
      # Check game score
      if game_total_score_diff[i] > -20:
        hm_count += 1
  # Away team
  away_count = 0
  for i in range(10):
    # Check bucket
    if (game_bucket_score_diff[i] < 0):
      # Check game score
      if game_total_score_diff[i] < 20:
        away_count += 1
  # season game_date	game_id	tm_code	tm	tm_div	opp_code	opp	opp_div	is_home
  hm_score = 100 * (hm_count / 10)
  away_score = 100 * (away_count / 10)
  
  hm_tm = game_id.split('-')[0]
  away_tm = game_id.split('-')[1]
  
  team_games.loc[index] = ['2018', hm_tm, away_tm, 0 if is_neutral == 1 else 1, hm_score]
  team_games.loc[index+1] = ['2018', away_tm, hm_tm, 0 if is_neutral == 1 else -1, away_score]
  index += 2

"""To compute adjusted team stats, we use [ridge regression from scikit-learn](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression). The idea behind it is that we think of each game-level stat as a function of 3 things: the team's ability, the opponent's ability, and [home-court advantage](https://en.wikipedia.org/wiki/Home_advantage). Using very loose model representation:

*game_stat ~ intercept + tm_rating + opp_rating + home_advantage + (error)*

We use the stat value from every game in a season, which includes matchups of various teams and opponents, and create [dummy variables](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) for both the team and opponent fields, with each set to 1 if the team is involved in the matchup (as "tm" or "opp"), otherwise 0. We can fit this model and use the coefficients to get team/opponent (and home advantage) estimates, with the regression *automatically* doing the opponent and site adjustment for us.

For each stat, the procedure generates a team "rating" that accounts for the strength of the opponent on the opposite side of the ball in the same stat (e.g. tm_efg_pct is adjusted for opponent's opp_efg_pct), as well as home advantage. This is a more thorough and statistically valid technique than the usual "stat vs opponent season average"-type adjustment that is commonly done in other such analysis.

Going beyond ordinary least squares, ridge regression uses a penalty term on the size of the coefficients to help handle multicollinearity and "shrink" coefficients (particularly in small sample size cases) - a form of [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)) that gives more sensible estimates. On a high level, hthis procedure gives the team ratings that "best fit" the game results given the model above, without going too extreme.

The cell below implements this ridge regression, extracting out the coefficients and adjusted values for each team, for each season-stat combination generated above. It gathers them in a "melted" data frame with 1 row per team-season per stat.
"""

#@title Use Ridge Regression to Get Adjusted Team Stats
# season game_date	game_id	tm_code	tm	tm_div	opp_code	opp	opp_div	is_home
# Create combo fields for season_tm & season_opp to be used in regression

## DEFINE season_stats
d = {'season': ['2018'], 'stat_name': ['competitiveness']}
season_stats = pd.DataFrame(data=d)


team_games['season_tm'] = (team_games['season'].map(str) + '_' + 
  team_games['tm_code'].map(str))

team_games['season_opp'] = (team_games['season'].map(str) + '_' + 
  team_games['opp_code'].map(str))


# Set up ridge regression model parameters
reg = linear_model.Ridge(alpha = 1, fit_intercept = True)

reg_results_collection = pd.DataFrame(columns = ['season', 'stat_name',
  'coef_name', 'ridge_reg_coef', 'ridge_reg_value'])

# Iterate through each season and stas
for index, row in season_stats.iterrows():
  this_season_game_stat = (team_games[team_games['season'] == row['season']]
    [['season_tm', 'tm_hca', 'season_opp', row['stat_name']]].
    reset_index()
    )

  this_season_game_dummy_vars = pd.get_dummies(
    this_season_game_stat[['season_tm', 'tm_hca', 'season_opp']]
    )

  # Fit ridge regression to given statistic using season game dummy variables
  reg.fit(
    X = this_season_game_dummy_vars,
    y = this_season_game_stat[row['stat_name']]
    )

  # Extract regression coefficients and put into data fram with coef names
  this_reg_results = pd.DataFrame(
    {
      # Add season and name of stat for this set of results
      'season': row['season'],
      'stat_name': row['stat_name'],
      # Coef name, which contains both season and tm_code
      'coef_name': this_season_game_dummy_vars.columns.values,
      # Coef that results from ridge regression
      'ridge_reg_coef': reg.coef_
    }
    )

  # Add intercept back in to reg coef to get 'adjusted' value
  this_reg_results['ridge_reg_value'] = (this_reg_results['ridge_reg_coef'] + 
    reg.intercept_
    )

  reg_results_collection = pd.concat([
    reg_results_collection,
    this_reg_results
    ],
    ignore_index = True
    )
  
reg_results_collection

# Only keep ratings from 'season_tm' perspective for this
tm_seasons_adjresults = (reg_results_collection[
  reg_results_collection['coef_name'].str.slice(0, 9) == 'season_tm'].
  rename(columns = {"ridge_reg_value": "adj_stat"}).
  reset_index(drop = True)
  )

# Convert season to integer
tm_seasons_adjresults['season'] = tm_seasons_adjresults['season'].map(int)

# Extract tm_code and convert back to integer
tm_seasons_adjresults['tm_code'] = (tm_seasons_adjresults['coef_name'].
  str.slice(15)).map(int)

tm_seasons_adjresults

#@title Get Raw Stats into df
# Metrics Df
metrics = pd.DataFrame(columns=['stat_name', 'rank_asc'])
metrics.loc[0] = ['competitiveness', False]

# team_seasons - tm_code, tm, season, competitiveness
team_seasons = pd.DataFrame(columns=['season', 'tm_code', 'tm', 'competitiveness'])
index = 0
for key, value in raw_rankings.items():
  team_seasons.loc[index] = [2018, int(key), team_code_to_school[key], value]
  index += 1
  
# Melt
team_seasons_stats_melt = pd.melt(
  team_seasons[['season', 'tm_code', 'tm'] + metrics['stat_name'].tolist()],
  id_vars = ['season', 'tm_code', 'tm'],
  value_vars = metrics['stat_name'].tolist(),
  var_name = 'stat_name',
  value_name = 'raw_stat'
  )

#@title Turn Each Stat into 0-100 Rating, Rank Each Team Season by Stat
# Merge raw and adjusted team stats
tm_seasons_raw_adj = pd.merge(
  team_seasons_stats_melt, 
  tm_seasons_adjresults.drop(['coef_name', 'ridge_reg_coef'], axis = 1),
  how = 'outer',
  on = ['season', 'tm_code', 'stat_name']
  )

# Mean/SD calculated across teams instead of games to count each team same;
# not weighting teams w/ more games (likely those that advanced further) more
season_stats_meansd = (tm_seasons_raw_adj.
  groupby(['season', 'stat_name'])['raw_stat', 'adj_stat'].
  agg(['mean', 'std']).
  reset_index()
  )

# Rename columns to get rid of hierarchical index
season_stats_meansd.columns = ['season', 'stat_name', 'season_raw_stat_mean', 
  'season_raw_stat_sd', 'season_adj_stat_mean', 'season_adj_stat_sd']

# Merge in season-level mean/sd, and whether metric ranks asc/desc
tm_seasons_stats_ranks = pd.merge(
  pd.merge(
    tm_seasons_raw_adj,
    season_stats_meansd,
    how = 'left',
    on = ['season', 'stat_name']
  ),  
  metrics[['stat_name', 'rank_asc']],
  how = 'left',
  on = ['stat_name']
  )

# Create field to help rank D1 teams only (not including group of Non-D1 teams)
tm_seasons_stats_ranks['rk_group'] = np.where(
  tm_seasons_stats_ranks['tm_code'] == -1, 'NON-D1', 'D1'
  )

# Loop over raw and adjusted version of each metric
for metric_type in ['raw', 'adj']:
  # Translate each stat to 0-100 'rating' by normalizing vs season mean/sd
  tm_seasons_stats_ranks[metric_type + '_rtg'] = stats.norm.cdf(
    np.where(tm_seasons_stats_ranks['rank_asc'], -1, 1) *
    (tm_seasons_stats_ranks[metric_type + '_stat'] 
      - tm_seasons_stats_ranks['season_' + metric_type + '_stat_mean']) /
     tm_seasons_stats_ranks['season_' + metric_type + '_stat_sd']
    ) * 100

  # Rank on rtg field, in correct dir, since 0 = worse & 100 = better by design
  tm_seasons_stats_ranks[metric_type + '_rk'] = np.where(
    # No rankings for group of Non-D1 teams
    tm_seasons_stats_ranks['rk_group'] == 'NON-D1', np.nan,
    (tm_seasons_stats_ranks.
    # Group by season, stat name, & rank group (so Non-D1 teams don't 'mix' in)
    groupby(['season', 'stat_name', 'rk_group'])[metric_type + '_rtg'].
    rank(ascending = False)
    ))

tm_seasons_stats_ranks

#@title Final Results
final_output = tm_seasons_stats_ranks.drop(['stat_name', 'season_raw_stat_mean', 'season_raw_stat_sd',
                             'season_adj_stat_mean', 'season_adj_stat_sd', 'rank_asc', 'rk_group'], axis = 1)

print('Competitiveness Rating')
print('----------------------')
print(final_output.sort_values(by=['adj_rtg'], ascending=False).to_string(index=False))

#@title Distribution of Competitiveness Scores
scores = list(final_output['adj_stat'])

plt.title("Frequency of each adj competitiveness scores")
plt.xlabel("Competitiveness score")
plt.ylabel("Frequency")
plt.grid(True)
plt.hist(scores)

print("     mean: %0.4f   var: %0.4f" % (np.mean(scores), np.var(scores, ddof=1)))
print("     min: %0.4f    max: %0.4f" % (np.min(scores), np.max(scores)))

"""## Results"""

adj_ratings = {}
raw_ratings = {}
for index, row in final_output.iterrows():
  if row['raw_rtg'] < 1:
    continue
  adj_ratings[row['tm_code']] = row['adj_rtg']
  raw_ratings[row['tm_code']] = row['raw_rtg']
  
SPACE_SIZE = 25
print("School", " "*(SPACE_SIZE - len("School")), "Adj Competitive Rating", " Raw Competitive Rating", "  Diff")
rank = 1
for team_code, score in sorted(adj_ratings.items(), key=operator.itemgetter(1), reverse=True):  
  team_name = team_code_to_school[team_code]
  spacing = SPACE_SIZE - len(team_name) - len(str(rank)) - 1
  print(rank, team_name, " "*spacing , '{0:.2f}'.format(score), " "*17, '{0:.2f}'.format(raw_ratings[team_code]), " "*17,
       '{0:.2f}'.format(score - raw_ratings[team_code]))
  rank += 1
# -*- coding: utf-8 -*-
"""Lead Changes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_Ds9l5dT6PPuSYRb1FA0o6PEgqPjbL2J
"""

from google.colab import auth
auth.authenticate_user()
print('Authenticated')

from google.cloud import bigquery
import pandas as pd
import matplotlib.pyplot as plt


client = bigquery.Client(project='stardust-analysis')

"""# Pre-processing"""

game_scoring_agg_df = client.query('''
SELECT *
FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                      OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                      OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                      OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                      OR game_date BETWEEN '2019-03-01' AND '2019-05-01')
ORDER BY tm_code, game_date
  ''').to_dataframe()

game_scoring_agg_df

seasons = [2014, 2015, 2016, 2017, 2018]
rolling_avg_df = pd.DataFrame(columns=list(game_scoring_agg_df.columns))

for season in seasons:
  
  temp_df = game_scoring_agg_df.loc[game_scoring_agg_df['season'] == season]
    
  new_stats = ['tm_pts', 'opp_pts', 'final_ptdiff', 'avg_ptdiff', 'ties', 'lead_changes', 'ties_and_lead_changes', 'max_lead', 'max_deficit', 'pct_time_tied', 'pct_time_leading', 'pct_time_up1to9', 'pct_time_up10plus', 'pct_time_up20plus', 'pct_time_trailing', 'pct_time_down1to9', 'pct_time_down10plus', 'pct_time_down20plus']
  
  for stat in new_stats:
    if stat == 'opp_pts':
      temp_df['pre_game_avg_pts_conceded'] = temp_df.groupby('tm_code')['opp_pts'].transform(lambda x: x.expanding().mean())
      temp_df.pre_game_avg_pts_conceded = temp_df.pre_game_avg_pts_conceded.shift(1)
      continue
    temp_df['pre_game_avg_{}'.format(stat)] = temp_df.groupby('tm_code')[stat].transform(lambda x: x.expanding().mean())
    temp_df['pre_game_avg_{}'.format(stat)] = temp_df['pre_game_avg_{}'.format(stat)].shift(1)
  
  frames = [rolling_avg_df, temp_df]
  
  rolling_avg_df = pd.concat(frames)

rolling_avg_df

tm_season_stats_df = client.query('''
SELECT DISTINCT end_date, x.tm_code, off_to_pct_adj, off_to_pct_adj_rk, def_to_pct_adj, def_to_pct_adj_rk, drb_pct_adj, drb_pct_adj_rk, orb_pct_adj, orb_pct_adj_rk, off_efg_pct_adj, off_efg_pct_adj_rk, def_efg_pct_adj, def_efg_pct_adj_rk, poss_per_40_adj, poss_per_40_adj_rk, off_ftm_rate_adj, off_ftm_rate_adj_rk, def_ftm_rate_adj, def_ftm_rate_adj_rk, game_id
FROM `stardust-analysis.ncaa_mbb.tm_season_stats_by_date_wide` as x
INNER JOIN (SELECT ties_and_lead_changes, tm_div, game_id, (DATE_ADD(game_date, INTERVAL -1 DAY)) as day_before, game_date, tm_code
            FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
            WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                                  OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                                  OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                                  OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                                  OR game_date BETWEEN '2019-03-01' AND '2019-05-01')) as y
ON x.end_date = y.day_before AND y.tm_code = x.tm_code
  ''').to_dataframe()

efficiencies_df = client.query('''
SELECT DISTINCT x.game_id, tm_pre_net_eff_raw, tm_pre_net_eff_adj, tm_pre_off_eff_raw, tm_pre_off_eff_adj, tm_pre_def_eff_raw, tm_pre_def_eff_adj, opp_pre_net_eff_raw, opp_pre_net_eff_adj, opp_pre_off_eff_raw, opp_pre_off_eff_adj, opp_pre_def_eff_raw, opp_pre_def_eff_adj, ties_and_lead_changes
FROM `stardust-analysis.ncaa_mbb.tm_game_stats_with_pregame_stats` as x
INNER JOIN (SELECT ties_and_lead_changes, tm_div, game_id
            FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
            WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                                  OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                                  OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                                  OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                                  OR game_date BETWEEN '2019-03-01' AND '2019-05-01')) as y
ON x.game_id = y.game_id
  ''').to_dataframe()

rolling_avg_cols = []
for stat in new_stats:
  if stat == 'opp_pts':
    rolling_avg_cols.append('pre_game_avg_pts_conceded')
    continue
  rolling_avg_cols.append('pre_game_avg_{}'.format(stat))

# all_colsz = list(efficiencies_df.columns)
# team_stats_colsz = []
# for col in rolling_avg_cols:
#   team_stats_colsz.append('team1_'+col)
#   team_stats_colsz.append('team2_'+col)
# all_colsz.extend(team_stats_colsz)
# all_colsz

# complete_dfz = efficiencies_df.reindex(columns=all_colsz)
# complete_dfz.columns

# for i in complete_dfz.index:
#   game_id = complete_dfz.at[i, 'game_id']
  
#   home_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'home_code'])
#   away_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'away_code'])
 
#   team1_stats = final_df[(final_df['tm_code'] == home_code) & (final_df['game_id'] == game_id )]
#   team2_stats = final_df[(final_df['tm_code'] == away_code) & (final_df['game_id'] == game_id )]
  
#   for stat in team_stats_colsz:
#     if stat[:5] == 'team1':
#       if not team1_stats.empty:
#         complete_dfz.at[i, stat] = team1_stats[stat[-(len(stat)-6):]]
#     else:
#       if not team2_stats.empty:
#         complete_dfz.at[i, stat] = team2_stats[stat[-(len(stat)-6):]]

# all_cols = list(df_eff.columns)
# team_stats_cols = []
# for col in new_cols:
#   team_stats_cols.append('team1_'+col)
#   team_stats_cols.append('team2_'+col)
# all_cols.extend(team_stats_cols)

eff_cols = list(efficiencies_df.columns)
tm_season_stats_cols = list(tm_season_stats_df.columns)
tm_season_stats_cols = [e for e in tm_season_stats_cols if e not in ('game_id', 'end_date', 'tm_code')]
net_stats_cols = []
net_eff_cols = []
net_season_cols = []
for col in eff_cols:
  if col == 'ties_and_lead_changes' or col == 'game_id':
    continue
  if 'net_'+col[-15:] not in net_eff_cols:
    net_eff_cols.append('net_'+col[-15:])
for col in rolling_avg_cols:
  net_stats_cols.append('net_'+col)

for col in tm_season_stats_cols:
  if col not in ['end_date', 'game_id', 'tm_code']:
    net_season_cols.append('net_'+col)
  
net_stats_cols.extend(net_eff_cols)
net_stats_cols.extend(net_season_cols)
net_stats_cols.extend(['game_id', 'ties_and_lead_changes'])

# net_eff_cols

complete_df = efficiencies_df.reindex(columns=net_stats_cols)

team_codes_df = client.query('''
SELECT DISTINCT z.game_id, home_code, away_code
FROM `stardust-analysis.ncaa_mbb.games` as z
INNER JOIN (
        SELECT DISTINCT x.game_id, tm_pre_net_eff_raw, opp_pre_net_eff_raw, ties_and_lead_changes
        FROM `stardust-analysis.ncaa_mbb.tm_game_stats_with_pregame_stats` as x
        INNER JOIN (SELECT ties_and_lead_changes, tm_div, game_id
                    FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
                    WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                                          OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                                          OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                                          OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                                          OR game_date BETWEEN '2019-03-01' AND '2019-05-01')) as y
        ON x.game_id = y.game_id
) as w
ON z.game_id = w.game_id
  ''').to_dataframe()

team_codes_df.tail(10)

complete_df = complete_df.drop_duplicates(subset=['game_id'], keep='first')

complete_df

backup_df = complete_df.copy()

efficiencies_df2 = client.query('''
SELECT DISTINCT x.game_id, tm_pre_net_eff_raw, tm_pre_net_eff_adj, tm_pre_off_eff_raw, tm_pre_off_eff_adj, tm_pre_def_eff_raw, tm_pre_def_eff_adj, opp_pre_net_eff_raw, opp_pre_net_eff_adj, opp_pre_off_eff_raw, opp_pre_off_eff_adj, opp_pre_def_eff_raw, opp_pre_def_eff_adj, ties_and_lead_changes, x.tm_code, x.opp_code, x.game_date
FROM `stardust-analysis.ncaa_mbb.tm_game_stats_with_pregame_stats` as x
INNER JOIN (SELECT ties_and_lead_changes, tm_div, game_id
            FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
            WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                                  OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                                  OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                                  OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                                  OR game_date BETWEEN '2019-03-01' AND '2019-05-01')) as y
ON x.game_id = y.game_id
ORDER BY x.game_date
  ''').to_dataframe()

tm_season_stats_df = client.query('''
SELECT DISTINCT end_date, x.tm_code, off_to_pct_adj, off_to_pct_adj_rk, def_to_pct_adj, def_to_pct_adj_rk, drb_pct_adj, drb_pct_adj_rk, orb_pct_adj, orb_pct_adj_rk, off_efg_pct_adj, off_efg_pct_adj_rk, def_efg_pct_adj, def_efg_pct_adj_rk, poss_per_40_adj, poss_per_40_adj_rk, off_ftm_rate_adj, off_ftm_rate_adj_rk, def_ftm_rate_adj, def_ftm_rate_adj_rk, game_id
FROM `stardust-analysis.ncaa_mbb.tm_season_stats_by_date_wide` as x
INNER JOIN (SELECT ties_and_lead_changes, tm_div, game_id, (DATE_ADD(game_date, INTERVAL -1 DAY)) as day_before, game_date, tm_code
            FROM `stardust-analysis.pbp_scoring.tm_game_pbp_scoring_agg` 
            WHERE tm_div = 1 AND (game_date BETWEEN '2015-03-01' AND '2015-05-01' 
                                  OR game_date BETWEEN '2016-03-01' AND '2016-05-01' 
                                  OR game_date BETWEEN '2017-03-01' AND '2017-05-01' 
                                  OR game_date BETWEEN '2018-03-01' AND '2018-05-01'
                                  OR game_date BETWEEN '2019-03-01' AND '2019-05-01')) as y
ON x.end_date = y.day_before AND y.tm_code = x.tm_code
  ''').to_dataframe()

efficiencies_df2

efficiencies_df2.tail(20)
final_4_efficiencies = efficiencies_df2.tail(34)

final_4_efficiencies

final_4_rolling_avgs = rolling_avg_df.sort_values(by=['game_date']).tail(25)
final_4_rolling_avgs

final_4_tm_season_stats_df = tm_season_stats_df.sort_values(by=['end_date']).tail(10)
final_4_tm_season_stats_df

final4df = efficiencies_df.reindex(columns=net_stats_cols).drop(['game_id', 'ties_and_lead_changes'], axis=1)
final4df.columns

matchups = [(746, 37), (416, 700), (746, 700), (746, 416), (416, 37), (700, 37)]

final4df['matchup'] = ''

i = 0
for matchup in matchups:
  
  final4df.at[i, 'matchup'] = str(matchup)
  
  team1_stats = final_4_efficiencies[(final_4_efficiencies['tm_code'] == matchup[0])]
  team2_stats = final_4_efficiencies[(final_4_efficiencies['opp_code'] == matchup[1])]
  
  for net_eff_col in net_eff_cols:
      final4df.at[i, net_eff_col] = abs(float(team1_stats['tm_'+net_eff_col[-15:]].iloc[len(team1_stats) - 1]) - float(team2_stats['opp_'+net_eff_col[-15:]].iloc[len(team2_stats) - 1]))
      
  
  team1_stats = final_4_rolling_avgs[(final_4_rolling_avgs['tm_code'] == matchup[0])]
  team2_stats = final_4_rolling_avgs[(final_4_rolling_avgs['tm_code'] == matchup[1])]
  
  for stat in rolling_avg_cols:
      final4df.at[i, 'net_'+stat] = abs(float((team1_stats[stat].iloc[len(team1_stats) - 1]) if len(team1_stats > 0) else team1_stats[stat]) - float((team2_stats[stat].iloc[len(team2_stats) - 1]) if len(team2_stats > 0) else team2_stats[stat]))
      
      
  
  team1_stats = final_4_tm_season_stats_df[(final_4_tm_season_stats_df['tm_code'] == matchup[0])]
  team2_stats = final_4_tm_season_stats_df[(final_4_tm_season_stats_df['tm_code'] == matchup[1])]
  
  for stat in tm_season_stats_cols:
      final4df.at[i, 'net_'+stat] = abs(float(team1_stats[stat]) - float(team2_stats[stat]))
  
      
  i += 1
                                     
final4df = final4df.head(6)
final4df

for i in complete_df.index:
  game_id = complete_df.at[i, 'game_id']
  
  eff_stats = efficiencies_df[efficiencies_df['game_id'] == game_id]
  
  for net_eff_col in net_eff_cols:
      complete_df.at[i, net_eff_col] = abs(float(eff_stats['tm_'+net_eff_col[-15:]].iloc[0]) - float(eff_stats['opp_'+net_eff_col[-15:]].iloc[0]))
  
  team1_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'home_code'])
  team2_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'away_code'])
 
  team1_pre_game_stats = rolling_avg_df[(rolling_avg_df['tm_code'] == team1_code) & (rolling_avg_df['game_id'] == game_id )][rolling_avg_cols]
  team2_pre_game_stats = rolling_avg_df[(rolling_avg_df['tm_code'] == team2_code) & (rolling_avg_df['game_id'] == game_id )][rolling_avg_cols]
  
  for stat in rolling_avg_cols:
    if not team1_pre_game_stats.empty and not team2_pre_game_stats.empty:
      complete_df.at[i, 'net_'+stat] = abs(float(team1_pre_game_stats[stat]) - float(team2_pre_game_stats[stat]))
      
  team1_pre_game_stats = tm_season_stats_df[(tm_season_stats_df['tm_code'] == team1_code) & (tm_season_stats_df['game_id'] == game_id )][tm_season_stats_cols]
  team2_pre_game_stats = tm_season_stats_df[(tm_season_stats_df['tm_code'] == team2_code) & (tm_season_stats_df['game_id'] == game_id )][tm_season_stats_cols]
  
  for stat in tm_season_stats_cols:
    if not team1_pre_game_stats.empty and not team2_pre_game_stats.empty:
      complete_df.at[i, 'net_'+stat] = abs(float(team1_pre_game_stats[stat]) - float(team2_pre_game_stats[stat]))

# for i in complete_df.index:
#   game_id = complete_df.at[i, 'game_id']
  
#   home_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'home_code'])
#   away_code = int(team_codes_df.loc[team_codes_df['game_id'] == game_id, 'away_code'])
 
#   team1_stats = final_df[(final_df['tm_code'] == home_code) & (final_df['game_id'] == game_id )]
#   team2_stats = final_df[(final_df['tm_code'] == away_code) & (final_df['game_id'] == game_id )]
  
#   for stat in team_stats_cols:
#     if stat[:5] == 'team1':
#       if not team1_stats.empty:
#         complete_df.at[i, stat] = team1_stats[stat[-(len(stat)-6):]]
#     else:
#       if not team2_stats.empty:
#         complete_df.at[i, stat] = team2_stats[stat[-(len(stat)-6):]]

# complete_df.to_csv('lead_changes_reduced.csv')
# from google.colab import files
# files.download('lead_changes_reduced.csv')

"""# MLP"""

mlp_df = complete_df.copy()
mlp_df.dropna(inplace=True)
mlp_df.shape

import math
from sklearn import datasets
from sklearn.datasets import load_files
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import numpy as np
import pandas 
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.metrics import classification_report,confusion_matrix

Y = poiss_df['ties_and_lead_changes']
X = poiss_df.ix[:,list(range(0,42))]

#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101)

X_train = X.head(7355)
y_train = Y.head(7355)
X_test = X.tail(2452)
y_test = Y.tail(2452)

sc = StandardScaler()
sc.fit(X_train)
X_train = sc.transform(X_train)
sc.fit(X_test)
X_test = sc.transform(X_test)

mlp_df.tail(100)

mlp = MLPClassifier(hidden_layer_sizes=(100,100,100,100,100),max_iter=500)
mlp.fit(X_train,y_train)
y_pred = mlp.predict(X_test)

print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, y_pred)

list(zip(y_test,y_pred))

prob = mlp.predict_proba(X_train)

list(zip(y_pred[:10], prob[:10], y_test[:10]))

mlp = MLPRegressor(hidden_layer_sizes=(100,100,100,100,100),max_iter=500)
mlp.fit(X_train,y_train)
y_pred = mlp.predict(X_test)

naive = [8.628687967369137] * len(y_test)
from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, naive)

"""# Poisson"""

poiss_df = complete_df.copy()
poiss_df.dropna(inplace=True)
poiss_df.shape

import matplotlib.pyplot as plt
plt.scatter(poiss_df['net_pre_net_eff_adj'], poiss_df['ties_and_lead_changes'])

poiss_df.drop(['game_id'], axis=1).corr(method='pearson').style.format("{:.2}").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)

dates_ids_df = client.query('''
SELECT game_id, game_date
FROM `stardust-analysis.ncaa_mbb.games`
WHERE game_date > '2014-11-01'
  ''').to_dataframe()

poiss_df["game_date"] = np.nan
for i in poiss_df.index:
  game_id = poiss_df.at[i, 'game_id']
  game_date = dates_ids_df.loc[dates_ids_df['game_id'] == game_id, 'game_date'].iloc[0]
  poiss_df.at[i, 'game_date'] = game_date

poiss_df.sort_values(by=['game_date'], inplace=True)
poiss_df

import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

Y = poiss_df['ties_and_lead_changes']
X = poiss_df.ix[:,list(range(0,42))]

#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101)

X_train = X.head(7355)
y_train = Y.head(7355)
X_test = X.tail(2452)
y_test = Y.tail(2452)

import xgboost as xgb

model = xgb.XGBRegressor()
model.fit(X_train, y_train)
model

xgb.plot_importance(model)
plt.show()

scores = list(zip(X_train.columns,model.feature_importances_))

from heapq import nlargest
from operator import itemgetter

best = nlargest(10, scores, key=itemgetter(1))
names, vals = zip(*best)
names = list(names)

model = xgb.XGBRegressor(learning_rate=0.1, n_estimators=230)
model.fit(X_train[names], y_train)
model

y_pred = model.predict(X_test[names])

from sklearn.metrics import mean_squared_error

naive = [np.mean(y_train)] * len(y_test)
mean_squared_error(y_test, naive)

plt.plot(range(len(y_test)), y_test, 'r*-', range(len(predVals)), predVals, 'bo-')
plt.title('Train dataset Real vs. Predicted Values')
plt.legend(['Real Values', 'Predicted Values'])
plt.show()

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, naive)

# from tpot import TPOTClassifier
# from tpot import TPOTRegressor


# tpot = TPOTClassifier(generations=5,verbosity=2)

# tpot.fit(X_train,y_train)

import statsmodels.api as sm

poisson_mod = sm.Poisson(y_train, X_train)
poisson_res = poisson_mod.fit(method="newton")
print(poisson_res.summary2())

# plt.scatter(y=poisson_mod.cdf(y_train))

# testing the model
predVals = poisson_res.predict(X_test)
naive = [8.628687967369137] * len(y_test)

plt.plot(range(len(y_test)), y_test, 'r*-', range(len(predVals)), predVals, 'bo-')
plt.title('Train dataset Real vs. Predicted Values')
plt.legend(['Real Values', 'Predicted Values'])
plt.show()

from sklearn.metrics import mean_squared_error
mean_squared_error(y_test, predVals)

# from statsmodels.discrete.discrete_model import Poisson
# poisson.cdf(X)

"""# Logistic Regression"""

log_df = complete_df.copy()
log_df.dropna(inplace=True)
log_df

more_than_x_lead_changes = client.query('''
SELECT game_id, ties_and_lead_changes, IF (ties_and_lead_changes > 6, 1, 0) as more_than_x_lead_changes
FROM `stardust-analysis.pbp_scoring.game_pbp_scoring_agg`''').to_dataframe()

np.mean(more_than_x_lead_changes['more_than_x_lead_changes'])

import numpy as np

log_df["more_than_x_lead_changes"] = np.nan

for i in log_df.index:
  game_id = log_df.at[i, 'game_id']
  log_df.at[i, "more_than_x_lead_changes"] = more_than_x_lead_changes.loc[more_than_x_lead_changes['game_id'] == game_id, 'more_than_x_lead_changes']

log_df["game_date"] = np.nan
for i in log_df.index:
  game_id = log_df.at[i, 'game_id']
  game_date = dates_ids_df.loc[dates_ids_df['game_id'] == game_id, 'game_date'].iloc[0]
  log_df.at[i, 'game_date'] = game_date

log_df.sort_values(by=['game_date'], inplace=True)
log_df

log_df.shape

log_df.drop('ties_and_lead_changes', axis=1, inplace=True)

log_df.columns

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

Y = log_df['more_than_x_lead_changes']
X = log_df.drop(['game_id', 'game_date', 'more_than_x_lead_changes'], axis=1)

log_df.shape

import xgboost as xgb
from sklearn.linear_model import LogisticRegression

X_train = X.head(2500)
y_train = Y.head(2500)
X_test = X.tail(708)
y_test = Y.tail(708)

# X_train = X.tail(557)
# y_train = Y.tail(557)

clf = LogisticRegression()
clf.fit(X_train, y_train)

model = xgb.XGBClassifier()
model.fit(X_train, y_train)

xgb.plot_importance(model)
plt.show()

y_pred = model.predict_proba(X_test)

clf_y_pred = clf.predict_proba(X_test)

np.mean(clf_y_pred)

naive_mean = np.mean(y_train)
naive_prob = [(1-naive_mean, naive_mean)] * len(y_test)
sse = 0
for tup in list(zip(y_test,clf_y_pred)):
  actual = tup[0]
  prob_0 = tup[1][0]
  prob_1 = tup[1][1]
  
  if actual == 1.0:
      err = 1 - prob_0
  else:
      err = 1 - prob_1
      
  sse += err
  
sse

y_pred = model.predict_proba(final4df.drop('matchup', axis=1))
'''
Texas Tech 700
Auburn 37
Virginia 746
MI State 416

37 vs 700
37 vs 416
746 vs 700
746 vs 416

'''

list(zip(y_pred, final4df['matchup']))

within_2poss = client.query('''
SELECT game_id, pct_time_within6, IF (pct_time_within6 > 50, 1, 0) as more_than_50_pct_within_2_poss
FROM `stardust-analysis.pbp_scoring.game_pbp_scoring_agg`''').to_dataframe()

within_2poss

import numpy as np

log_df["more_than_50_pct_within_2_poss"] = np.nan

for i in log_df.index:
  game_id = log_df.at[i, 'game_id']
  log_df.at[i, "more_than_50_pct_within_2_poss"] = within_2poss.loc[within_2poss['game_id'] == game_id, 'more_than_50_pct_within_2_poss']

log_df["game_date"] = np.nan
for i in log_df.index:
  game_id = log_df.at[i, 'game_id']
  game_date = dates_ids_df.loc[dates_ids_df['game_id'] == game_id, 'game_date'].iloc[0]
  log_df.at[i, 'game_date'] = game_date

log_df.sort_values(by=['game_date'], inplace=True)
log_df

log_df.shape

log_df.drop('ties_and_lead_changes', axis=1, inplace=True)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

Y = log_df['more_than_50_pct_within_2_poss']

#X = log_df.ix[:,list(range(0,42))]
X = log_df.drop(['game_id', 'game_date', 'more_than_50_pct_within_2_poss'], axis=1)

#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=101)

# X_train = X.head(8000)
# y_train = Y.head(8000)
# X_test = X.tail(1807)
# y_test = Y.tail(1807)

# clf = LogisticRegression().fit(X_train, y_train)
# predictions = clf.predict(X_test)
# predictions_prob = clf.predict_proba(X_test)

# adj_predictions = []
# for pred in predictions_prob:
#   if pred[1] > 0.5:
#     adj_predictions.append(1)
#   else:
#     adj_predictions.append(0)
    

# print(accuracy_score(predictions, y_test))
# np.mean(predictions)
#list(predictions_prob)
#list(zip(predictions, predictions_prob))

X.shape

from sklearn.decomposition import PCA

pca = PCA(n_components='mle', svd_solver='full')

X = pd.DataFrame(pca.fit_transform(X))

X.shape

import xgboost as xgb

X_train = X.head(750)
y_train = Y.head(750)
X_test = X.tail(151)
y_test = Y.tail(151)

# X_train = X.tail(557)
# y_train = Y.tail(557)

model = xgb.XGBClassifier()
model.fit(X_train, y_train)
model

xgb.plot_importance(model)
plt.show()

final4df.shape

final4df
pd.options.display.max_columns = None
display(final4df)

totaled = final4df.copy()
totaled['Total'] = totaled.iloc[:, -43:-1].sum(axis=1)
display(totaled)

y_pred = model.predict_proba(final4df.drop('matchup', axis=1))
#y_pred = model.predict_proba(X_test)
y_pred

list(zip(y_pred, final4df['matchup']))

print(accuracy_score(y_pred, y_test))
np.mean(y_pred)

naive_mean = np.mean(y_train)
naive_prob = [(1-naive_mean, naive_mean)] * len(y_test)
sse = 0
for tup in list(zip(y_test,y_pred)):
  actual = tup[0]
  prob_0 = tup[1][0]
  prob_1 = tup[1][1]
  
  if prob_1 > 0.5:
    if actual == 1.0:
      err = 1 - prob_0
    else:
      err = 1 - prob_1
  else:
    if actual == 1.0:
      err = 1 - prob_0
    else:
      err = 1 - prob_1
      
  sse += err
  
sse

from sklearn.feature_selection import SelectFromModel

model = SelectFromModel(clf, prefit=True)

X_new = model.transform(X)

feature_idx = model.get_support()
X.columns[feature_idx]

log_df.drop(['game_id'], axis=1).corr(method='pearson').style.format("{:.2}").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)